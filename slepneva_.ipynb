{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f914cd3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ae83701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ –°–û–ó–î–ê–ù–ò–ï –î–ê–¢–ê–°–ï–¢–ê V1: RAW DATA\n",
      "ClearML results page: https://app.clear.ml/projects/c6b27df2c1d343f5b8ded148b93dfe5e/experiments/039ed9d78d9e49b5a7efbd604c8ce0b5/output/log\n",
      "ClearML dataset page: https://app.clear.ml/datasets/simple/c6b27df2c1d343f5b8ded148b93dfe5e/experiments/039ed9d78d9e49b5a7efbd604c8ce0b5\n",
      "Displaying metadata in the UI is only supported for pandas Dataframes for now. Skipping!\n",
      "Uploading dataset changes (1 files compressed to 9.9 MiB) to https://files.clear.ml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100% | 9.90/9.9 MB [00:02<00:00,  3.84MB/s]: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File compression and upload completed: total size 9.9 MiB, 1 chunk(s) stored (average size 9.9 MiB)\n",
      "‚úÖ Dataset V1 —Å–æ–∑–¥–∞–Ω: 039ed9d78d9e49b5a7efbd604c8ce0b5\n",
      "\n",
      "üîπ –°–û–ó–î–ê–ù–ò–ï –î–ê–¢–ê–°–ï–¢–ê V2: TRAIN/TEST SPLITS\n",
      "ClearML results page: https://app.clear.ml/projects/6c22464c761a410b84f5e312a13b6663/experiments/3520fae4f5044740872128c239c09ff9/output/log\n",
      "ClearML dataset page: https://app.clear.ml/datasets/simple/6c22464c761a410b84f5e312a13b6663/experiments/3520fae4f5044740872128c239c09ff9\n",
      "Displaying metadata in the UI is only supported for pandas Dataframes for now. Skipping!\n",
      "Uploading dataset changes (2 files compressed to 3.86 MiB) to https://files.clear.ml\n",
      "File compression and upload completed: total size 3.86 MiB, 1 chunk(s) stored (average size 3.86 MiB)\n",
      "‚úÖ Dataset V2 —Å–æ–∑–¥–∞–Ω: 3520fae4f5044740872128c239c09ff9\n",
      "   –ù–∞—Å–ª–µ–¥—É–µ—Ç –æ—Ç: 039ed9d78d9e49b5a7efbd604c8ce0b5\n",
      "\n",
      "============================================================\n",
      "‚úÖ –í–°–ï –î–ê–¢–ê–°–ï–¢–´ –°–û–ó–î–ê–ù–´ –ò –í–ï–†–°–ò–û–ù–ò–†–û–í–ê–ù–´!\n",
      "============================================================\n",
      "V1 (Raw):      039ed9d78d9e49b5a7efbd604c8ce0b5\n",
      "V2 (Train/Test): 3520fae4f5044740872128c239c09ff9\n",
      "Lineage: V1 ‚Üí V2\n"
     ]
    }
   ],
   "source": [
    "# 01_create_datasets.py\n",
    "from clearml import Task, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# ======= 1. –°–æ–∑–¥–∞–µ–º Task –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏ =======\n",
    "task = Task.init(\n",
    "    project_name=\"Customer_Return_Prediction\",\n",
    "    task_name=\"Dataset Creation and Versioning\",\n",
    "    task_type=Task.TaskTypes.data_processing\n",
    ")\n",
    "\n",
    "# –õ–æ–≥–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
    "task.set_parameter(\"dataset_versioning\", {\n",
    "    \"raw_data_source\": \"transaction_.csv\",\n",
    "    \"train_test_split\": \"temporal_split\",\n",
    "    \"train_period\": \"until 2019-09-01\",\n",
    "    \"test_period\": \"2019-09-01 to 2019-10-01\"\n",
    "})\n",
    "\n",
    "# ======= 2. –í–ï–†–°–ò–Ø 1: –°—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ (–º–∏–Ω–∏–º–∞–ª—å–Ω–æ –æ—á–∏—â–µ–Ω–Ω—ã–µ) =======\n",
    "print(\"üîπ –°–û–ó–î–ê–ù–ò–ï –î–ê–¢–ê–°–ï–¢–ê V1: RAW DATA\")\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ –æ—á–∏—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "transaction = pd.read_csv('transaction_.csv')\n",
    "rename_dict = {'clientID': 'client', 'trDte': 'visit_date', 'itemGroup': 'item_group'}\n",
    "transaction = transaction.rename(columns=rename_dict)\n",
    "transaction['visit_date'] = pd.to_datetime(transaction['visit_date'], format='%d.%m.%Y')\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "os.makedirs('datasets', exist_ok=True)\n",
    "transaction.to_csv('datasets/raw_cleaned.csv', index=False)\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç V1\n",
    "dataset_v1 = Dataset.create(\n",
    "    dataset_name=\"customer_data_raw\",\n",
    "    dataset_project=\"Customer_Return_Prediction\",\n",
    "    dataset_tags=[\"v1.0\", \"raw\", \"cleaned\"]\n",
    ")\n",
    "\n",
    "# –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: –ø–µ—Ä–µ–¥–∞–µ–º —Å—Ç—Ä–æ–∫—É, –∞ –Ω–µ —Å–ø–∏—Å–æ–∫\n",
    "dataset_v1.add_files('datasets/raw_cleaned.csv')\n",
    "dataset_v1.set_metadata({\n",
    "    \"description\": \"–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ –æ—á–∏—â–µ–Ω–Ω—ã–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\",\n",
    "    \"processing_steps\": [\"loading\", \"column_renaming\", \"date_parsing\"],\n",
    "    \"rows\": len(transaction),\n",
    "    \"columns\": list(transaction.columns),\n",
    "    \"date_range\": f\"{transaction['visit_date'].min().date()} to {transaction['visit_date'].max().date()}\"\n",
    "})\n",
    "\n",
    "dataset_v1.upload()\n",
    "dataset_v1.finalize()\n",
    "\n",
    "print(f\"‚úÖ Dataset V1 —Å–æ–∑–¥–∞–Ω: {dataset_v1.id}\")\n",
    "task.upload_artifact(\"raw_data_sample\", transaction.head(100))\n",
    "\n",
    "# ======= 3. –í–ï–†–°–ò–Ø 2: –û–±—É—á–∞—é—â–∏–µ –∏ —Ç–µ—Å—Ç–æ–≤—ã–µ –≤—ã–±–æ—Ä–∫–∏ =======\n",
    "print(\"\\nüîπ –°–û–ó–î–ê–ù–ò–ï –î–ê–¢–ê–°–ï–¢–ê V2: TRAIN/TEST SPLITS\")\n",
    "\n",
    "# –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–∞—à—É –ª–æ–≥–∏–∫—É –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è train/test\n",
    "def calculate_client_profile_at_date(df, observation_end_date):\n",
    "    \"\"\"–í–∞—à–∞ —Ñ—É–Ω–∫—Ü–∏—è –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –∫–æ–¥–∞\"\"\"\n",
    "    obs_date = pd.to_datetime(observation_end_date)\n",
    "    visits = df[df['visit_date'] < obs_date].copy()\n",
    "    \n",
    "    if visits.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    visits['is_weekend'] = visits['visit_date'].dt.dayofweek >= 5\n",
    "    \n",
    "    agg = visits.groupby('client').agg(\n",
    "        last_visit=('visit_date', 'max'),\n",
    "        visits=('visit_date', 'count'),\n",
    "        unique_visits=('visit_date', 'nunique'),\n",
    "        amount=('amount', 'sum'),\n",
    "        quantity=('quantity', 'sum'),\n",
    "        items=('item', 'nunique'),\n",
    "        weekends=('is_weekend', 'sum')\n",
    "    ).reset_index()\n",
    "    \n",
    "    agg['Recency'] = (obs_date - agg['last_visit']).dt.days\n",
    "    agg['Frequency'] = agg['unique_visits']\n",
    "    agg['Monetary'] = agg['amount']\n",
    "    agg['avg_check'] = agg['amount'] / agg['Frequency']\n",
    "    agg['avg_items'] = agg['quantity'] / agg['Frequency']\n",
    "    \n",
    "    last_amount = visits.loc[visits.groupby('client')['visit_date'].idxmax()]\n",
    "    agg = agg.merge(last_amount[['client', 'amount']].rename(\n",
    "        columns={'amount': 'last_visit_amount'}), on='client')\n",
    "    \n",
    "    return agg\n",
    "\n",
    "def mark_events(df, result_start_date, result_end_date):\n",
    "    \"\"\"–í–∞—à–∞ —Ñ—É–Ω–∫—Ü–∏—è —Ä–∞–∑–º–µ—Ç–∫–∏ —Å–æ–±—ã—Ç–∏–π\"\"\"\n",
    "    start_date = pd.to_datetime(result_start_date)\n",
    "    end_date = pd.to_datetime(result_end_date)\n",
    "    \n",
    "    base_clients = df['client'].unique()\n",
    "    period_mask = (df['visit_date'] >= start_date) & (df['visit_date'] < end_date)\n",
    "    active_clients = df.loc[period_mask, 'client'].unique()\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'client': base_clients,\n",
    "        'event': [1 if client in active_clients else 0 for client in base_clients]\n",
    "    })\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º train –¥–∞–Ω–Ω—ã–µ\n",
    "train_profile = calculate_client_profile_at_date(transaction, '2019-09-01')\n",
    "train_events = mark_events(transaction, '2019-09-01', '2019-10-01')\n",
    "train_df = train_profile.merge(train_events, on='client', how='inner')\n",
    "train_df['event'] = train_df['event'].astype(int)\n",
    "train_df = train_df.drop(columns=[col for col in train_df.columns if 'date' in col.lower()], errors='ignore')\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º test –¥–∞–Ω–Ω—ã–µ\n",
    "test_profile = calculate_client_profile_at_date(transaction, '2019-10-01')\n",
    "test_events = mark_events(transaction, '2019-10-01', '2019-11-01')\n",
    "test_df = test_profile.merge(test_events, on='client', how='inner')\n",
    "test_df['event'] = test_df['event'].astype(int)\n",
    "test_df = test_df.drop(columns=[col for col in test_df.columns if 'date' in col.lower()], errors='ignore')\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª—ã\n",
    "train_df.to_csv('datasets/train_data_v2.csv', index=False)\n",
    "test_df.to_csv('datasets/test_data_v2.csv', index=False)\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç V2 —Å –Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ–º –æ—Ç V1\n",
    "dataset_v2 = Dataset.create(\n",
    "    dataset_name=\"customer_data_train_test\",\n",
    "    dataset_project=\"Customer_Return_Prediction\",\n",
    "    dataset_tags=[\"v2.0\", \"processed\", \"train_test_split\"],\n",
    "    parent_datasets=[dataset_v1.id]  # –ù–∞—Å–ª–µ–¥—É–µ–º –æ—Ç V1!\n",
    ")\n",
    "\n",
    "# –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: –¥–æ–±–∞–≤–ª—è–µ–º —Ñ–∞–π–ª—ã –ø–æ –æ–¥–Ω–æ–º—É\n",
    "dataset_v2.add_files('datasets/train_data_v2.csv')\n",
    "dataset_v2.add_files('datasets/test_data_v2.csv')\n",
    "\n",
    "# –ò–ª–∏ –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –≤—Å—é –ø–∞–ø–∫—É:\n",
    "# dataset_v2.add_files('datasets/')\n",
    "\n",
    "dataset_v2.set_metadata({\n",
    "    \"description\": \"–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å train/test —Ä–∞–∑–±–∏–µ–Ω–∏–µ–º\",\n",
    "    \"parent_dataset\": dataset_v1.id,\n",
    "    \"train_samples\": len(train_df),\n",
    "    \"test_samples\": len(test_df),\n",
    "    \"features_count\": len(train_df.columns) - 1,  # –±–µ–∑ target\n",
    "    \"target_distribution_train\": {\n",
    "        \"returned\": int(train_df['event'].sum()),\n",
    "        \"not_returned\": int(len(train_df) - train_df['event'].sum()),\n",
    "        \"return_rate\": float(train_df['event'].mean())\n",
    "    },\n",
    "    \"target_distribution_test\": {\n",
    "        \"returned\": int(test_df['event'].sum()),\n",
    "        \"not_returned\": int(len(test_df) - test_df['event'].sum()),\n",
    "        \"return_rate\": float(test_df['event'].mean())\n",
    "    }\n",
    "})\n",
    "\n",
    "dataset_v2.upload()\n",
    "dataset_v2.finalize()\n",
    "\n",
    "print(f\"‚úÖ Dataset V2 —Å–æ–∑–¥–∞–Ω: {dataset_v2.id}\")\n",
    "print(f\"   –ù–∞—Å–ª–µ–¥—É–µ—Ç –æ—Ç: {dataset_v1.id}\")\n",
    "\n",
    "# ======= 4. –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≤ Task =======\n",
    "task.upload_artifact(\"train_data_sample\", train_df.head(100))\n",
    "task.upload_artifact(\"test_data_sample\", test_df.head(100))\n",
    "\n",
    "stats = {\n",
    "    \"datasets_created\": {\n",
    "        \"v1_raw\": dataset_v1.id,\n",
    "        \"v2_train_test\": dataset_v2.id\n",
    "    },\n",
    "    \"data_statistics\": {\n",
    "        \"raw_rows\": len(transaction),\n",
    "        \"train_rows\": len(train_df),\n",
    "        \"test_rows\": len(test_df),\n",
    "        \"train_features\": list(train_df.columns),\n",
    "        \"train_target_balance\": f\"{train_df['event'].mean():.1%}\",\n",
    "        \"test_target_balance\": f\"{test_df['event'].mean():.1%}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "task.upload_artifact(\"dataset_statistics\", stats)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ –í–°–ï –î–ê–¢–ê–°–ï–¢–´ –°–û–ó–î–ê–ù–´ –ò –í–ï–†–°–ò–û–ù–ò–†–û–í–ê–ù–´!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"V1 (Raw):      {dataset_v1.id}\")\n",
    "print(f\"V2 (Train/Test): {dataset_v2.id}\")\n",
    "print(f\"Lineage: V1 ‚Üí V2\")\n",
    "\n",
    "task.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "264140cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Can't get url information for git repo in C:\\Users\\Ksenia\\AppData\\Roaming\\Python\\Python311\\site-packages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=58f1a07bd9f74a22881b7e2417b1469b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Can't get branch information for git repo in C:\\Users\\Ksenia\\AppData\\Roaming\\Python\\Python311\\site-packages\n",
      "Can't get commit information for git repo in C:\\Users\\Ksenia\\AppData\\Roaming\\Python\\Python311\\site-packages\n",
      "Can't get diff information for git repo in C:\\Users\\Ksenia\\AppData\\Roaming\\Python\\Python311\\site-packages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML results page: https://app.clear.ml/projects/ebbc67863e744623a4f3e7490e757670/experiments/58f1a07bd9f74a22881b7e2417b1469b/output/log\n",
      "üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ ClearML Dataset...\n",
      "ClearML Monitor: GPU monitoring failed getting GPU reading, switching off GPU monitoring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100% | 9.90/9.9 MB [00:01<00:00,  9.51MB/s]: C:\\Users\\Ksenia\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\std.py:636: TqdmWarning:\n",
      "\n",
      "clamping frac to range [0, 1]\n",
      "\n",
      "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100% | 9.90/9.9 MB [00:01<00:00,  9.46MB/s]: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–î–∞–Ω–Ω—ã–µ —Å–∫–∞—á–∞–Ω—ã –≤: C:/Users/Ksenia/.clearml/cache/storage_manager/datasets/ds_3520fae4f5044740872128c239c09ff9\n",
      "Train data: (39906, 15)\n",
      "Test data: (41196, 15)\n",
      "‚úÖ –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω—ã –≤ ClearML\n",
      "\n",
      "üîß –û–±—É—á–µ–Ω–∏–µ Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ksenia\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä –ú–ï–¢–†–ò–ö–ò –ú–û–î–ï–õ–ò:\n",
      "ROC-AUC: 0.7980\n",
      "Accuracy: 0.8274\n",
      "Precision: 0.6385\n",
      "Recall: 0.1966\n",
      "F1-Score: 0.3006\n",
      "\n",
      "üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...\n",
      "–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: models/logistic_regression_model.pkl\n",
      "\n",
      "============================================================\n",
      "‚úÖ –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ –ó–ê–í–ï–†–®–ï–ù!\n",
      "============================================================\n",
      "Task ID: 58f1a07bd9f74a22881b7e2417b1469b\n",
      "Dataset used: 3520fae4f5044740872128c239c09ff9\n",
      "Model saved: models/logistic_regression_model.pkl\n",
      "ROC-AUC: 0.7980\n"
     ]
    }
   ],
   "source": [
    "# 02_logistic_regression_experiment.py\n",
    "from clearml import Task, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# ======= 1. –°–æ–∑–¥–∞–µ–º Task –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ =======\n",
    "task = Task.init(\n",
    "    project_name=\"Customer_Return_Prediction\",\n",
    "    task_name=\"Logistic Regression Experiment\",\n",
    "    task_type=Task.TaskTypes.training\n",
    ")\n",
    "\n",
    "# ======= 2. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç V2 —á–µ—Ä–µ–∑ ClearML =======\n",
    "print(\"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ ClearML Dataset...\")\n",
    "dataset_v2 = Dataset.get(\n",
    "    dataset_name=\"customer_data_train_test\",\n",
    "    dataset_project=\"Customer_Return_Prediction\",\n",
    "    dataset_tags=[\"v2.0\", \"processed\", \"train_test_split\"],\n",
    "    only_completed=True\n",
    ")\n",
    "\n",
    "# –°–∫–∞—á–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ª–æ–∫–∞–ª—å–Ω–æ\n",
    "local_path = dataset_v2.get_local_copy()\n",
    "print(f\"–î–∞–Ω–Ω—ã–µ —Å–∫–∞—á–∞–Ω—ã –≤: {local_path}\")\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º train/test —Ñ–∞–π–ª—ã\n",
    "train_path = os.path.join(local_path, 'train_data_v2.csv')\n",
    "test_path = os.path.join(local_path, 'test_data_v2.csv')\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "print(f\"Train data: {train_df.shape}\")\n",
    "print(f\"Test data: {test_df.shape}\")\n",
    "\n",
    "# ======= 3. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö =======\n",
    "X_train = train_df.drop(columns=['client', 'event'], errors='ignore')\n",
    "y_train = train_df['event']\n",
    "X_test = test_df.drop(columns=['client', 'event'], errors='ignore')\n",
    "y_test = test_df['event']\n",
    "\n",
    "# –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "X_train = X_train.select_dtypes(include=[np.number])\n",
    "X_test = X_test.select_dtypes(include=[np.number])\n",
    "\n",
    "# –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –∫–æ–ª–æ–Ω–æ–∫\n",
    "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "# ======= 4. –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ =======\n",
    "hyperparameters = {\n",
    "    \"model_type\": \"LogisticRegression\",\n",
    "    \"penalty\": \"l2\",           # L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è\n",
    "    \"C\": 1.0,                  # –û–±—Ä–∞—Ç–Ω–∞—è —Å–∏–ª–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏\n",
    "    \"solver\": \"lbfgs\",         # –ê–ª–≥–æ—Ä–∏—Ç–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏\n",
    "    \"max_iter\": 1000,          # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π\n",
    "    \"random_state\": 42,\n",
    "    \"class_weight\": None,      # –í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤\n",
    "    \"fit_intercept\": True      # –î–æ–±–∞–≤–ª—è—Ç—å intercept\n",
    "}\n",
    "\n",
    "# –õ–æ–≥–∏—Ä—É–µ–º –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ ClearML (3.4)\n",
    "task.connect(hyperparameters)\n",
    "print(\"‚úÖ –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω—ã –≤ ClearML\")\n",
    "\n",
    "# ======= 5. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ =======\n",
    "print(\"\\nüîß –û–±—É—á–µ–Ω–∏–µ Logistic Regression...\")\n",
    "model = LogisticRegression(\n",
    "    penalty=hyperparameters[\"penalty\"],\n",
    "    C=hyperparameters[\"C\"],\n",
    "    solver=hyperparameters[\"solver\"],\n",
    "    max_iter=hyperparameters[\"max_iter\"],\n",
    "    random_state=hyperparameters[\"random_state\"],\n",
    "    class_weight=hyperparameters[\"class_weight\"],\n",
    "    fit_intercept=hyperparameters[\"fit_intercept\"]\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ======= 6. –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫–∞ =======\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "accuracy = (y_pred == y_test).mean()\n",
    "precision = ((y_pred == 1) & (y_test == 1)).sum() / max((y_pred == 1).sum(), 1)\n",
    "recall = ((y_pred == 1) & (y_test == 1)).sum() / max(y_test.sum(), 1)\n",
    "f1 = 2 * precision * recall / max(precision + recall, 1e-10)\n",
    "\n",
    "# –õ–æ–≥–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≤ ClearML (3.5)\n",
    "task.get_logger().report_scalar(\n",
    "    title=\"Model Metrics\",\n",
    "    series=\"ROC-AUC\",\n",
    "    value=roc_auc,\n",
    "    iteration=0\n",
    ")\n",
    "\n",
    "task.get_logger().report_scalar(\n",
    "    title=\"Model Metrics\",\n",
    "    series=\"Accuracy\",\n",
    "    value=accuracy,\n",
    "    iteration=0\n",
    ")\n",
    "\n",
    "task.get_logger().report_scalar(\n",
    "    title=\"Model Metrics\", \n",
    "    series=\"F1-Score\",\n",
    "    value=f1,\n",
    "    iteration=0\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä –ú–ï–¢–†–ò–ö–ò –ú–û–î–ï–õ–ò:\")\n",
    "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "# ======= 7. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è =======\n",
    "# ROC-–∫—Ä–∏–≤–∞—è\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, label=f'Logistic Regression (AUC = {roc_auc:.3f})', linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Model', alpha=0.5)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Logistic Regression')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "task.get_logger().report_matplotlib_figure(\n",
    "    title=\"ROC Curve\",\n",
    "    series=\"Logistic Regression\",\n",
    "    figure=plt.gcf(),\n",
    "    iteration=0\n",
    ")\n",
    "plt.close()\n",
    "\n",
    "# –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "task.get_logger().report_matplotlib_figure(\n",
    "    title=\"Confusion Matrix\",\n",
    "    series=\"Logistic Regression\", \n",
    "    figure=plt.gcf(),\n",
    "    iteration=0\n",
    ")\n",
    "plt.close()\n",
    "\n",
    "# ======= 8. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∫–∞–∫ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–∞ (3.3) =======\n",
    "print(\"\\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...\")\n",
    "os.makedirs('models', exist_ok=True)\n",
    "model_filename = 'models/logistic_regression_model.pkl'\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–µ—Ä–µ–∑ joblib (–ª—É—á—à–µ –¥–ª—è sklearn –º–æ–¥–µ–ª–µ–π)\n",
    "joblib.dump(model, model_filename)\n",
    "print(f\"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_filename}\")\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–∞–∫ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç –≤ ClearML\n",
    "task.upload_artifact(\n",
    "    name=\"logistic_regression_model\",\n",
    "    artifact_object=model_filename,\n",
    "    metadata={\n",
    "        \"model_type\": \"LogisticRegression\",\n",
    "        \"hyperparameters\": hyperparameters,\n",
    "        \"metrics\": {\n",
    "            \"roc_auc\": roc_auc,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"f1_score\": f1\n",
    "        },\n",
    "        \"dataset_version\": dataset_v2.id\n",
    "    }\n",
    ")\n",
    "\n",
    "# –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —á–µ—Ä–µ–∑ pickle –¥–ª—è –ø–æ–ª–Ω–æ—Ç—ã\n",
    "with open('models/logistic_regression_model_pickle.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "# ======= 9. –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ =======\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'coefficient': abs(model.coef_[0])\n",
    "}).sort_values('coefficient', ascending=False)\n",
    "\n",
    "task.upload_artifact(\"feature_importance\", feature_importance)\n",
    "\n",
    "# ======= 10. –ò—Ç–æ–≥–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è =======\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ –ó–ê–í–ï–†–®–ï–ù!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Task ID: {task.id}\")\n",
    "print(f\"Dataset used: {dataset_v2.id}\")\n",
    "print(f\"Model saved: {model_filename}\")\n",
    "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "task.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b07274d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Can't get url information for git repo in C:\\Users\\Ksenia\\AppData\\Roaming\\Python\\Python311\\site-packages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=eb2fabf954da4fd1bac6f2017e55231f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Can't get branch information for git repo in C:\\Users\\Ksenia\\AppData\\Roaming\\Python\\Python311\\site-packages\n",
      "Can't get commit information for git repo in C:\\Users\\Ksenia\\AppData\\Roaming\\Python\\Python311\\site-packages\n",
      "Can't get diff information for git repo in C:\\Users\\Ksenia\\AppData\\Roaming\\Python\\Python311\\site-packages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML results page: https://app.clear.ml/projects/ebbc67863e744623a4f3e7490e757670/experiments/eb2fabf954da4fd1bac6f2017e55231f/output/log\n",
      "üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ ClearML Dataset...\n",
      "ClearML Monitor: GPU monitoring failed getting GPU reading, switching off GPU monitoring\n",
      "‚úÖ –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã XGBoost –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω—ã\n",
      "\n",
      "üîß –û–±—É—á–µ–Ω–∏–µ XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ksenia\\AppData\\Roaming\\Python\\Python311\\site-packages\\xgboost\\training.py:199: UserWarning:\n",
      "\n",
      "[18:31:49] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä –ú–ï–¢–†–ò–ö–ò XGBoost:\n",
      "ROC-AUC: 0.8075\n",
      "Accuracy: 0.8337\n",
      "\n",
      "üîç –°–†–ê–í–ù–ï–ù–ò–ï –° LOGISTIC REGRESSION:\n",
      "(–í —Ä–µ–∞–ª—å–Ω–æ–º —Å—Ü–µ–Ω–∞—Ä–∏–∏ –Ω—É–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ Task)\n",
      "XGBoost –æ–±—ã—á–Ω–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ksenia\\AppData\\Roaming\\Python\\Python311\\site-packages\\clearml\\utilities\\plotlympl\\renderer.py:209: UserWarning:\n",
      "\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ XGBoost –º–æ–¥–µ–ª–∏...\n",
      "\n",
      "============================================================\n",
      "üìù –î–õ–Ø –ü–û–Ø–°–ù–ò–¢–ï–õ–¨–ù–û–ô –ó–ê–ü–ò–°–ö–ò:\n",
      "============================================================\n",
      "\n",
      "–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –≤–∞—Ä—å–∏—Ä–æ–≤–∞–ª–∏—Å—å:\n",
      "1. n_estimators: 100 ‚Üí 150 (—É–≤–µ–ª–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–µ—Ä–µ–≤—å–µ–≤)\n",
      "2. max_depth: 3 ‚Üí 5 (—É–≤–µ–ª–∏—á–µ–Ω–∏–µ –≥–ª—É–±–∏–Ω—ã –¥–µ—Ä–µ–≤—å–µ–≤)\n",
      "3. learning_rate: 0.1 ‚Üí 0.05 (—É–º–µ–Ω—å—à–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è)\n",
      "4. –î–æ–±–∞–≤–ª–µ–Ω—ã —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏: reg_alpha, reg_lambda\n",
      "5. –î–æ–±–∞–≤–ª–µ–Ω—ã: subsample, colsample_bytree\n",
      "\n",
      "–ö–∞–∫ –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–æ –≤ ClearML:\n",
      "- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω Task.connect() –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
      "- –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–∏–¥–Ω—ã –≤–æ –≤–∫–ª–∞–¥–∫–µ Configuration\n",
      "- –ö–∞–∂–¥—ã–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –∏–º–µ–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
      "\n",
      "–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (–Ω—É–∂–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –æ–±–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞):\n",
      "1. Logistic Regression: –ø—Ä–æ—â–µ, –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º, –±—ã—Å—Ç—Ä–µ–µ\n",
      "2. XGBoost: —Å–ª–æ–∂–Ω–µ–µ, –æ–±—ã—á–Ω–æ –ª—É—á—à–µ —Ç–æ—á–Ω–æ—Å—Ç—å, –º–µ–¥–ª–µ–Ω–Ω–µ–µ –æ–±—É—á–µ–Ω–∏–µ\n",
      "3. –°—Ä–∞–≤–Ω–∏—Ç—å ROC-AUC –∏–∑ –æ–±–æ–∏—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤\n",
      "4. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ –æ–±–µ–∏—Ö –º–æ–¥–µ–ª—è—Ö\n",
      "\n",
      "============================================================\n",
      "‚úÖ –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ XGBoost –ó–ê–í–ï–†–®–ï–ù!\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying (Retry(total=237, connect=237, read=240, redirect=240, status=240)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000014A86C80DD0>: Failed to establish a new connection: [WinError 10065] –°–¥–µ–ª–∞–Ω–∞ –ø–æ–ø—ã—Ç–∫–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –æ–ø–µ—Ä–∞—Ü–∏—é –Ω–∞ —Å–æ–∫–µ—Ç–µ –¥–ª—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ–≥–æ —Ö–æ—Å—Ç–∞')': /v2.23/tasks.ping\n",
      "Retrying (Retry(total=237, connect=237, read=240, redirect=240, status=240)) after connection broken by 'NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000014AE1499050>: Failed to resolve 'api.clear.ml' ([Errno 11001] getaddrinfo failed)\")': /v2.23/tasks.ping\n",
      "Retrying (Retry(total=236, connect=236, read=240, redirect=240, status=240)) after connection broken by 'NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000014A82B7A810>: Failed to resolve 'api.clear.ml' ([Errno 11001] getaddrinfo failed)\")': /v2.23/tasks.ping\n",
      "Retrying (Retry(total=235, connect=235, read=240, redirect=240, status=240)) after connection broken by 'NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000014AE14918D0>: Failed to resolve 'api.clear.ml' ([Errno 11001] getaddrinfo failed)\")': /v2.23/tasks.ping\n",
      "Retrying (Retry(total=237, connect=237, read=240, redirect=240, status=240)) after connection broken by 'NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000014A86CA1450>: Failed to resolve 'api.clear.ml' ([Errno 11001] getaddrinfo failed)\")': /v2.23/tasks.ping\n",
      "Retrying (Retry(total=236, connect=236, read=240, redirect=240, status=240)) after connection broken by 'NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000014A86C91CD0>: Failed to resolve 'api.clear.ml' ([Errno 11001] getaddrinfo failed)\")': /v2.23/tasks.ping\n",
      "Retrying (Retry(total=235, connect=235, read=240, redirect=240, status=240)) after connection broken by 'NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000014A818F6A90>: Failed to resolve 'api.clear.ml' ([Errno 11001] getaddrinfo failed)\")': /v2.23/tasks.ping\n",
      "Retrying (Retry(total=234, connect=234, read=240, redirect=240, status=240)) after connection broken by 'NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000014A818CBFD0>: Failed to resolve 'api.clear.ml' ([Errno 11001] getaddrinfo failed)\")': /v2.23/tasks.ping\n",
      "Retrying (Retry(total=233, connect=233, read=240, redirect=240, status=240)) after connection broken by 'NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000014AEFCBBD90>: Failed to resolve 'api.clear.ml' ([Errno 11001] getaddrinfo failed)\")': /v2.23/tasks.ping\n",
      "Retrying (Retry(total=232, connect=232, read=240, redirect=240, status=240)) after connection broken by 'NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000014A8081CF10>: Failed to resolve 'api.clear.ml' ([Errno 11001] getaddrinfo failed)\")': /v2.23/tasks.ping\n",
      "Retrying (Retry(total=231, connect=231, read=240, redirect=240, status=240)) after connection broken by 'NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000014A8081F290>: Failed to resolve 'api.clear.ml' ([Errno 11001] getaddrinfo failed)\")': /v2.23/tasks.ping\n",
      "Retrying (Retry(total=230, connect=230, read=240, redirect=240, status=240)) after connection broken by 'NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000014A8081C990>: Failed to resolve 'api.clear.ml' ([Errno 11001] getaddrinfo failed)\")': /v2.23/tasks.ping\n",
      "Retrying (Retry(total=229, connect=229, read=240, redirect=240, status=240)) after connection broken by 'NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000014A86C9A510>: Failed to resolve 'api.clear.ml' ([Errno 11001] getaddrinfo failed)\")': /v2.23/tasks.ping\n",
      "Retrying (Retry(total=228, connect=228, read=240, redirect=240, status=240)) after connection broken by 'NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000014A86D5FB90>: Failed to resolve 'api.clear.ml' ([Errno 11001] getaddrinfo failed)\")': /v2.23/tasks.ping\n",
      "Retrying (Retry(total=227, connect=227, read=240, redirect=240, status=240)) after connection broken by 'NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000014A87A65550>: Failed to resolve 'api.clear.ml' ([Errno 11001] getaddrinfo failed)\")': /v2.23/tasks.ping\n",
      "Retrying (Retry(total=226, connect=226, read=240, redirect=240, status=240)) after connection broken by 'NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000014A81977650>: Failed to resolve 'api.clear.ml' ([Errno 11001] getaddrinfo failed)\")': /v2.23/tasks.ping\n",
      "Retrying (Retry(total=225, connect=225, read=240, redirect=240, status=240)) after connection broken by 'NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000014A81C99950>: Failed to resolve 'api.clear.ml' ([Errno 11001] getaddrinfo failed)\")': /v2.23/tasks.ping\n",
      "Retrying (Retry(total=224, connect=224, read=240, redirect=240, status=240)) after connection broken by 'NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000014AE14EC850>: Failed to resolve 'api.clear.ml' ([Errno 11001] getaddrinfo failed)\")': /v2.23/tasks.ping\n",
      "Retrying (Retry(total=223, connect=223, read=240, redirect=240, status=240)) after connection broken by 'NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000014AE1491B50>: Failed to resolve 'api.clear.ml' ([Errno 11001] getaddrinfo failed)\")': /v2.23/tasks.ping\n",
      "Retrying (Retry(total=222, connect=222, read=240, redirect=240, status=240)) after connection broken by 'NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000014A818C12D0>: Failed to resolve 'api.clear.ml' ([Errno 11001] getaddrinfo failed)\")': /v2.23/tasks.ping\n"
     ]
    }
   ],
   "source": [
    "# 03_xgboost_experiment.py\n",
    "from clearml import Task, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# ======= 1. –°–æ–∑–¥–∞–µ–º Task –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ =======\n",
    "task = Task.init(\n",
    "    project_name=\"Customer_Return_Prediction\", \n",
    "    task_name=\"XGBoost Experiment\",\n",
    "    task_type=Task.TaskTypes.training\n",
    ")\n",
    "\n",
    "# ======= 2. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç V2 =======\n",
    "print(\"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ ClearML Dataset...\")\n",
    "dataset_v2 = Dataset.get(\n",
    "    dataset_name=\"customer_data_train_test\",\n",
    "    dataset_project=\"Customer_Return_Prediction\",\n",
    "    dataset_tags=[\"v2.0\", \"processed\", \"train_test_split\"],\n",
    "    only_completed=True\n",
    ")\n",
    "\n",
    "local_path = dataset_v2.get_local_copy()\n",
    "train_path = os.path.join(local_path, 'train_data_v2.csv')\n",
    "test_path = os.path.join(local_path, 'test_data_v2.csv')\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# ======= 3. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö =======\n",
    "X_train = train_df.drop(columns=['client', 'event'], errors='ignore')\n",
    "y_train = train_df['event']\n",
    "X_test = test_df.drop(columns=['client', 'event'], errors='ignore')\n",
    "y_test = test_df['event']\n",
    "\n",
    "X_train = X_train.select_dtypes(include=[np.number])\n",
    "X_test = X_test.select_dtypes(include=[np.number])\n",
    "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "# ======= 4. –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã XGBoost =======\n",
    "hyperparameters = {\n",
    "    \"model_type\": \"XGBClassifier\",\n",
    "    \"n_estimators\": 150,           # –£–≤–µ–ª–∏—á–∏–ª–∏ —Å 100 (–≤ –ø—Ä–æ—à–ª–æ–º –∫–æ–¥–µ)\n",
    "    \"max_depth\": 5,                # –£–≤–µ–ª–∏—á–∏–ª–∏ —Å 3\n",
    "    \"learning_rate\": 0.05,         # –£–º–µ–Ω—å—à–∏–ª–∏ —Å 0.1\n",
    "    \"subsample\": 0.8,              # –î–æ–±–∞–≤–∏–ª–∏ subsample\n",
    "    \"colsample_bytree\": 0.8,       # –î–æ–±–∞–≤–∏–ª–∏ colsample\n",
    "    \"reg_alpha\": 0.1,              # L1 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è\n",
    "    \"reg_lambda\": 1.0,             # L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è\n",
    "    \"random_state\": 42,\n",
    "    \"eval_metric\": \"logloss\",\n",
    "    \"use_label_encoder\": False\n",
    "}\n",
    "\n",
    "# –õ–æ–≥–∏—Ä—É–µ–º –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã (3.4)\n",
    "task.connect(hyperparameters)\n",
    "print(\"‚úÖ –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã XGBoost –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω—ã\")\n",
    "\n",
    "# ======= 5. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ =======\n",
    "print(\"\\nüîß –û–±—É—á–µ–Ω–∏–µ XGBoost...\")\n",
    "model = XGBClassifier(\n",
    "    n_estimators=hyperparameters[\"n_estimators\"],\n",
    "    max_depth=hyperparameters[\"max_depth\"],\n",
    "    learning_rate=hyperparameters[\"learning_rate\"],\n",
    "    subsample=hyperparameters[\"subsample\"],\n",
    "    colsample_bytree=hyperparameters[\"colsample_bytree\"],\n",
    "    reg_alpha=hyperparameters[\"reg_alpha\"],\n",
    "    reg_lambda=hyperparameters[\"reg_lambda\"],\n",
    "    random_state=hyperparameters[\"random_state\"],\n",
    "    eval_metric=hyperparameters[\"eval_metric\"],\n",
    "    use_label_encoder=hyperparameters[\"use_label_encoder\"]\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ======= 6. –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ =======\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "accuracy = (y_pred == y_test).mean()\n",
    "\n",
    "# –õ–æ–≥–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ (3.5)\n",
    "task.get_logger().report_scalar(\n",
    "    title=\"Model Metrics\",\n",
    "    series=\"ROC-AUC\",\n",
    "    value=roc_auc,\n",
    "    iteration=0\n",
    ")\n",
    "\n",
    "task.get_logger().report_scalar(\n",
    "    title=\"Model Metrics\",\n",
    "    series=\"Accuracy\", \n",
    "    value=accuracy,\n",
    "    iteration=0\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä –ú–ï–¢–†–ò–ö–ò XGBoost:\")\n",
    "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# ======= 7. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å Logistic Regression =======\n",
    "# –ú–æ–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞\n",
    "print(\"\\nüîç –°–†–ê–í–ù–ï–ù–ò–ï –° LOGISTIC REGRESSION:\")\n",
    "print(\"(–í —Ä–µ–∞–ª—å–Ω–æ–º —Å—Ü–µ–Ω–∞—Ä–∏–∏ –Ω—É–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ Task)\")\n",
    "print(\"XGBoost –æ–±—ã—á–Ω–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\")\n",
    "\n",
    "# ======= 8. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è =======\n",
    "# ROC-–∫—Ä–∏–≤–∞—è\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, label=f'XGBoost (AUC = {roc_auc:.3f})', linewidth=2, color='green')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Model', alpha=0.5)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - XGBoost')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "task.get_logger().report_matplotlib_figure(\n",
    "    title=\"ROC Curve\",\n",
    "    series=\"XGBoost\",\n",
    "    figure=plt.gcf(),\n",
    "    iteration=0\n",
    ")\n",
    "plt.close()\n",
    "\n",
    "# –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(importance_df['feature'][:15], importance_df['importance'][:15])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 15 Feature Importance - XGBoost')\n",
    "plt.gca().invert_yaxis()\n",
    "task.get_logger().report_matplotlib_figure(\n",
    "    title=\"Feature Importance\",\n",
    "    series=\"XGBoost\",\n",
    "    figure=plt.gcf(),\n",
    "    iteration=0\n",
    ")\n",
    "plt.close()\n",
    "\n",
    "# ======= 9. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ (3.3) =======\n",
    "print(\"\\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ XGBoost –º–æ–¥–µ–ª–∏...\")\n",
    "model_filename = 'models/xgboost_model.pkl'\n",
    "joblib.dump(model, model_filename)\n",
    "\n",
    "task.upload_artifact(\n",
    "    name=\"xgboost_model\",\n",
    "    artifact_object=model_filename,\n",
    "    metadata={\n",
    "        \"model_type\": \"XGBClassifier\",\n",
    "        \"hyperparameters\": hyperparameters,\n",
    "        \"metrics\": {\n",
    "            \"roc_auc\": roc_auc,\n",
    "            \"accuracy\": accuracy\n",
    "        },\n",
    "        \"dataset_version\": dataset_v2.id,\n",
    "        \"top_features\": importance_df['feature'].head(5).tolist()\n",
    "    }\n",
    ")\n",
    "\n",
    "# ======= 10. –í—ã–≤–æ–¥—ã –¥–ª—è –ø–æ—è—Å–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∑–∞–ø–∏—Å–∫–∏ =======\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìù –î–õ–Ø –ü–û–Ø–°–ù–ò–¢–ï–õ–¨–ù–û–ô –ó–ê–ü–ò–°–ö–ò:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –≤–∞—Ä—å–∏—Ä–æ–≤–∞–ª–∏—Å—å:\")\n",
    "print(\"1. n_estimators: 100 ‚Üí 150 (—É–≤–µ–ª–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–µ—Ä–µ–≤—å–µ–≤)\")\n",
    "print(\"2. max_depth: 3 ‚Üí 5 (—É–≤–µ–ª–∏—á–µ–Ω–∏–µ –≥–ª—É–±–∏–Ω—ã –¥–µ—Ä–µ–≤—å–µ–≤)\")\n",
    "print(\"3. learning_rate: 0.1 ‚Üí 0.05 (—É–º–µ–Ω—å—à–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è)\")\n",
    "print(\"4. –î–æ–±–∞–≤–ª–µ–Ω—ã —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏: reg_alpha, reg_lambda\")\n",
    "print(\"5. –î–æ–±–∞–≤–ª–µ–Ω—ã: subsample, colsample_bytree\")\n",
    "\n",
    "print(\"\\n–ö–∞–∫ –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–æ –≤ ClearML:\")\n",
    "print(\"- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω Task.connect() –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\")\n",
    "print(\"- –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–∏–¥–Ω—ã –≤–æ –≤–∫–ª–∞–¥–∫–µ Configuration\")\n",
    "print(\"- –ö–∞–∂–¥—ã–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –∏–º–µ–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\")\n",
    "\n",
    "print(\"\\n–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (–Ω—É–∂–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –æ–±–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞):\")\n",
    "print(\"1. Logistic Regression: –ø—Ä–æ—â–µ, –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º, –±—ã—Å—Ç—Ä–µ–µ\")\n",
    "print(\"2. XGBoost: —Å–ª–æ–∂–Ω–µ–µ, –æ–±—ã—á–Ω–æ –ª—É—á—à–µ —Ç–æ—á–Ω–æ—Å—Ç—å, –º–µ–¥–ª–µ–Ω–Ω–µ–µ –æ–±—É—á–µ–Ω–∏–µ\")\n",
    "print(\"3. –°—Ä–∞–≤–Ω–∏—Ç—å ROC-AUC –∏–∑ –æ–±–æ–∏—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤\")\n",
    "print(\"4. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ –æ–±–µ–∏—Ö –º–æ–¥–µ–ª—è—Ö\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ XGBoost –ó–ê–í–ï–†–®–ï–ù!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "task.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
